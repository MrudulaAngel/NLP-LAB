{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsInbbyAUd9GZCJpEMNnTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrudulaAngel/NLP-LAB/blob/main/LAB_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kugodn-kdvPl"
      },
      "outputs": [],
      "source": [
        "#NAME:MALLAVARAPU MRUDULA ANGEL\n",
        "#REGISTRATION NUMBER:21BCE9934"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 1\n",
        "#Write a progarm to slit the sentences in a document\n",
        "import nltk\n",
        "\n",
        "def split_sentences(document):\n",
        "    # Download the necessary NLTK resources\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    # Use the sent_tokenize() function to split the document into sentences\n",
        "    sentences = nltk.sent_tokenize(document)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/story.txt\"\n",
        "with open(file_path, 'r') as file:\n",
        "    document = file.read()\n",
        "\n",
        "sentences = split_sentences(document)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPMJQi_Kd0kG",
        "outputId": "5f27a847-dd58-4a93-bf79-740ff25a2ae3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is an extremely popular story about a hare and a tortoise.', 'The hare is an animal that is known to move quickly, while a tortoise is one to move slowly.', 'One day, the hare challenged the tortoise to a race simply to prove that he was the best.', 'The tortoise agreed.', 'Once the race began the hare was easily able to get a head start.', 'Upon realizing that the tortoise is far behind.', 'The overconfident hare decided to take a nap.', 'Meanwhile the tortoise, who was extremely determined and dedicated to the race was slowly nearing the finish line.', 'The tortoise won the race while the hare napped.', 'Most importantly he did it with humility and without arrogance.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 2\n",
        "#Write a paragraph to perform tokenization and stemming by reading input string or paragraph\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Initialize the PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Perform stemming on each word\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Example usage\n",
        "input_text = input(\"Enter a string or paragraph: \")\n",
        "tokenized_and_stemmed_text = tokenize_and_stem(input_text)\n",
        "print(tokenized_and_stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vYl7ANjeMfx",
        "outputId": "3e90a92d-791e-4cc0-d5f4-295d40c1763a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a string or paragraph: This is an extremely popular story about a hare and a tortoise.\n",
            "['thi', 'is', 'an', 'extrem', 'popular', 'stori', 'about', 'a', 'hare', 'and', 'a', 'tortois', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 3\n",
        "#Remove the stopwords and rare words in a document\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def remove_rare_words(tokens, threshold):\n",
        "    # Count the frequency of each word\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # Filter out rare words\n",
        "    filtered_tokens = [token for token in tokens if word_counts[token] >= threshold]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Read the document file\n",
        "file_path = '/content/story.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    document_text = file.read()\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = remove_stopwords(document_text)\n",
        "\n",
        "# Remove rare words\n",
        "threshold = 2\n",
        "filtered_tokens = remove_rare_words(filtered_tokens, threshold)\n",
        "\n",
        "# Print the filtered tokens\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV13479LeqLX",
        "outputId": "391d72cd-0ff1-48cc-fe3b-d75bc2156627"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['extremely', 'hare', 'tortoise', '.', 'hare', 'move', ',', 'tortoise', 'move', 'slowly', '.', ',', 'hare', 'tortoise', 'race', '.', 'tortoise', '.', 'race', 'hare', '.', 'tortoise', '.', 'hare', '.', 'tortoise', ',', 'extremely', 'race', 'slowly', '.', 'tortoise', 'race', 'hare', '.', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 4\n",
        "#Write a program to display parts of speech in a document\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def display_parts_of_speech(document_path):\n",
        "    # Read the content of the document from the text file\n",
        "    with open(document_path, 'r') as file:\n",
        "        document = file.read()\n",
        "\n",
        "    # Tokenize the document into words\n",
        "    words = word_tokenize(document)\n",
        "\n",
        "    # Perform part-of-speech tagging\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    # Display the parts of speech\n",
        "    print(\"Parts of Speech:\")\n",
        "    for word, pos_tag in pos_tags:\n",
        "        print(f\"{word}: {pos_tag}\")\n",
        "\n",
        "# Path to the text file\n",
        "document_path = '/content/story.txt'  # Replace 'your_document.txt' with the path to your text file\n",
        "\n",
        "# Display parts of speech in the document\n",
        "display_parts_of_speech(document_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ojPek9hffa",
        "outputId": "37a6f3df-d2ce-466b-8616-e358cf5a7b11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech:\n",
            "This: DT\n",
            "is: VBZ\n",
            "an: DT\n",
            "extremely: RB\n",
            "popular: JJ\n",
            "story: NN\n",
            "about: IN\n",
            "a: DT\n",
            "hare: NN\n",
            "and: CC\n",
            "a: DT\n",
            "tortoise: NN\n",
            ".: .\n",
            "The: DT\n",
            "hare: NN\n",
            "is: VBZ\n",
            "an: DT\n",
            "animal: NN\n",
            "that: WDT\n",
            "is: VBZ\n",
            "known: VBN\n",
            "to: TO\n",
            "move: VB\n",
            "quickly: RB\n",
            ",: ,\n",
            "while: IN\n",
            "a: DT\n",
            "tortoise: NN\n",
            "is: VBZ\n",
            "one: CD\n",
            "to: TO\n",
            "move: VB\n",
            "slowly: RB\n",
            ".: .\n",
            "One: CD\n",
            "day: NN\n",
            ",: ,\n",
            "the: DT\n",
            "hare: NN\n",
            "challenged: VBD\n",
            "the: DT\n",
            "tortoise: NN\n",
            "to: TO\n",
            "a: DT\n",
            "race: NN\n",
            "simply: RB\n",
            "to: TO\n",
            "prove: VB\n",
            "that: IN\n",
            "he: PRP\n",
            "was: VBD\n",
            "the: DT\n",
            "best: JJS\n",
            ".: .\n",
            "The: DT\n",
            "tortoise: NN\n",
            "agreed: VBD\n",
            ".: .\n",
            "Once: IN\n",
            "the: DT\n",
            "race: NN\n",
            "began: VBD\n",
            "the: DT\n",
            "hare: NN\n",
            "was: VBD\n",
            "easily: RB\n",
            "able: JJ\n",
            "to: TO\n",
            "get: VB\n",
            "a: DT\n",
            "head: JJ\n",
            "start: NN\n",
            ".: .\n",
            "Upon: IN\n",
            "realizing: VBG\n",
            "that: IN\n",
            "the: DT\n",
            "tortoise: NN\n",
            "is: VBZ\n",
            "far: RB\n",
            "behind: RB\n",
            ".: .\n",
            "The: DT\n",
            "overconfident: NN\n",
            "hare: NN\n",
            "decided: VBD\n",
            "to: TO\n",
            "take: VB\n",
            "a: DT\n",
            "nap: NN\n",
            ".: .\n",
            "Meanwhile: RB\n",
            "the: DT\n",
            "tortoise: NN\n",
            ",: ,\n",
            "who: WP\n",
            "was: VBD\n",
            "extremely: RB\n",
            "determined: JJ\n",
            "and: CC\n",
            "dedicated: VBN\n",
            "to: TO\n",
            "the: DT\n",
            "race: NN\n",
            "was: VBD\n",
            "slowly: RB\n",
            "nearing: VBG\n",
            "the: DT\n",
            "finish: JJ\n",
            "line: NN\n",
            ".: .\n",
            "The: DT\n",
            "tortoise: NN\n",
            "won: VBD\n",
            "the: DT\n",
            "race: NN\n",
            "while: IN\n",
            "the: DT\n",
            "hare: NN\n",
            "napped: VBN\n",
            ".: .\n",
            "Most: JJS\n",
            "importantly: RB\n",
            "he: PRP\n",
            "did: VBD\n",
            "it: PRP\n",
            "with: IN\n",
            "humility: NN\n",
            "and: CC\n",
            "without: IN\n",
            "arrogance: NN\n",
            ".: .\n"
          ]
        }
      ]
    }
  ]
}